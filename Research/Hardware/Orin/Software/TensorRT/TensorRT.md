# TensorRT

TensorRT is NVIDIA’s high-performance deep learning inference optimizer and runtime, delivering up to 36× faster inference compared to CPU-only systems. It supports post-training and quantization-aware training techniques for FP8, FP4, and INT formats, ideal for real-time and edge AI applications.

## Key Features

- ⚡ **Optimized Inference**: Quantization, layer fusion, and kernel tuning.
- 🤖 **LLM Acceleration**: TensorRT-LLM boosts LLMs with a simplified Python API.
- ☁️ **TensorRT Cloud**: CLI-based cloud optimization (limited access).
- 🧠 **Model Optimizer**: Integrates quantization, sparsity, and distillation.
- 🔗 **Framework Integrations**: Works with PyTorch, Hugging Face, ONNX, MATLAB.
- 🚀 **Deployment**: Scales with NVIDIA Triton Inference Server.
- 🌍 **Platform Support**: Jetson, DRIVE, RTX, Tesla – from edge to data center.

🔗 [TensorRT Official Site](https://developer.nvidia.com/tensorrt)  
📚 [Documentation](https://docs.nvidia.com/tensorrt/)
