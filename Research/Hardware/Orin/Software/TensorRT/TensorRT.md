# TensorRT

TensorRT is NVIDIAâ€™s high-performance deep learning inference optimizer and runtime, delivering up to 36Ã— faster inference compared to CPU-only systems. It supports post-training and quantization-aware training techniques for FP8, FP4, and INT formats, ideal for real-time and edge AI applications.

## Key Features

- âš¡ **Optimized Inference**: Quantization, layer fusion, and kernel tuning.
- ğŸ¤– **LLM Acceleration**: TensorRT-LLM boosts LLMs with a simplified Python API.
- â˜ï¸ **TensorRT Cloud**: CLI-based cloud optimization (limited access).
- ğŸ§  **Model Optimizer**: Integrates quantization, sparsity, and distillation.
- ğŸ”— **Framework Integrations**: Works with PyTorch, Hugging Face, ONNX, MATLAB.
- ğŸš€ **Deployment**: Scales with NVIDIA Triton Inference Server.
- ğŸŒ **Platform Support**: Jetson, DRIVE, RTX, Tesla â€“ from edge to data center.

ğŸ”— [TensorRT Official Site](https://developer.nvidia.com/tensorrt)  
ğŸ“š [Documentation](https://docs.nvidia.com/tensorrt/)
