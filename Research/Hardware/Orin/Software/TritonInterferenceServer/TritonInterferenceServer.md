# Triton Interference Server 
https://github.com/triton-inference-server


## Description
NVIDIA Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.

This top level GitHub organization host repositories for officially supported backends, including TensorRT, TensorFlow, PyTorch, Python, ONNX Runtime, and OpenVino. The organization also hosts several popular Triton tools, including:

    Model Analyzer: A tool to analyze the runtime performance of a model and provide an optimized model configuration for Triton Inference Server.
    https://github.com/triton-inference-server/model_analyzer

    Model Navigator: a tool that provides the ability to automate the process of moving a model from source to optimal format and configuration for deployment on Triton Inference Server.
    https://github.com/triton-inference-server/model_navigator


## Deep Learning Examples

https://github.com/NVIDIA/DeepLearningExamples

### Getting Started

https://github.com/triton-inference-server/server/tree/main/docs

https://github.com/triton-inference-server/server#documentation

https://github.com/triton-inference-server

